{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT V3\n",
    "\n",
    "单个模型(fixed), 直接对接到webui (生成很多个), 所以产生一行一个的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import gradio as gr\n",
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "# Assuming DEVICE is already defined\n",
    "DEVICE = 'cuda'  # Use 'cuda' for GPU or 'cpu' for CPU\n",
    "\n",
    "# Path to the models' directories\n",
    "MODEL_DIRS = [\n",
    "    'pixiv-prompts-gpt-finetunes/8xh100_run2_e2_s50k',\n",
    "]\n",
    "\n",
    "# Load models and tokenizers from directories\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "for model_dir in MODEL_DIRS:\n",
    "    model_name = os.path.basename(os.path.normpath(model_dir))\n",
    "    models[model_name] = transformers.GPT2LMHeadModel.from_pretrained(model_dir).to(DEVICE)\n",
    "    tokenizers[model_name] = transformers.AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "\n",
    "# Hardcoded model name\n",
    "HARDCODED_MODEL_NAME = os.path.basename(os.path.normpath(MODEL_DIRS[0]))\n",
    "\n",
    "def generate_text(rating: str, date: str, quality: str, character: str, prompt: str, max_length: int, num_lines: int, selected_models: list):\n",
    "    \"\"\"Generate text based on input prompt for selected models, managing multiple lines output.\"\"\"\n",
    "    query_prompt = f'<input rating=\"{rating}\" chara=\"{character}\" date=\"{date}\" quality=\"{quality}\" tags=\"{prompt}\">'\n",
    "    query_prompt += \"<output>\"\n",
    "\n",
    "    all_outputs = []\n",
    "    for model_name in selected_models:\n",
    "        model = models[model_name].eval()  # Set the model to evaluation mode\n",
    "        tokenizer = tokenizers[model_name]\n",
    "        for _ in tqdm(range(num_lines)):\n",
    "            input_ids = tokenizer.encode(query_prompt, return_tensors='pt').to(DEVICE)\n",
    "            output_sequences = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=max_length + len(input_ids[0]),\n",
    "                temperature=1.0,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.0,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "\n",
    "            generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "            # Find and remove the initial part up to <output>\n",
    "            start_output = generated_text.find(\"<output>\") + len(\"<output>\")\n",
    "            generated_text = generated_text[start_output:].strip()\n",
    "\n",
    "            # Remove the ending </output> tag or truncate at last complete tag\n",
    "            end_tag = generated_text.find(\"</output>\")\n",
    "            if end_tag != -1:\n",
    "                generated_text = generated_text[:end_tag]\n",
    "            else:\n",
    "                last_comma = generated_text.rfind(\",\")\n",
    "                if last_comma != -1:\n",
    "                    generated_text = generated_text[:last_comma]\n",
    "\n",
    "            all_outputs.append(generated_text.strip())\n",
    "\n",
    "    # Join all outputs into a single string separated by new lines\n",
    "    return \"\\n\".join(all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_OUTPUT_DIR = \"generated_images\"\n",
    "\n",
    "\n",
    "def save_images_and_metadata(gend_images, gend_text, prompt):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(IMG_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    for image in gend_images:\n",
    "        # Get filename\n",
    "        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        img_hash = hashlib.md5(image.tobytes()).hexdigest()        \n",
    "        file_name = f\"{timestamp}_{img_hash[:4]}.webp\"        \n",
    "        file_path = os.path.join(IMG_OUTPUT_DIR, file_name)\n",
    "        \n",
    "        # Save image\n",
    "        image.save(file_path, format='WEBP')\n",
    "\n",
    "        # Create metadata JSON\n",
    "        metadata = {\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_text\": gend_text,\n",
    "        }\n",
    "        json_file_name = f\"{timestamp}_{img_hash[:4]}.json\"\n",
    "        json_file_path = os.path.join(IMG_OUTPUT_DIR, json_file_name)\n",
    "        \n",
    "        with open(json_file_path, 'w') as json_file:\n",
    "            json.dump(metadata, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://13d7d6124540429eda.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://13d7d6124540429eda.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77dea2da368f4bbe999f40bccca0d699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text 0: 1girl, green eyes, smile, maid headdress, looking at viewer, puffy short sleeves, eyebrows visible through hair, bangs, hair ornament, black dress, very long hair, white apron, alternate costume, enmaided, closed mouth, frilled apron, waist apron, maid apron, white background, hair between eyes, cleavage cutout, medium breasts, clothing cutout, genshin impact, maid outfit\n",
      "Generated text 1: 1girl, brown hair, hair ornament, large breasts, short hair, open mouth, yellow eyes, hairclip, sitting, looking at viewer, collarbone, cleavage, outdoors, black swimsuit, hair between eyes, black bikini, bangs, holding towel, covered navel, bare shoulders, thighs, bench, wet clothes, one-piece swimsuit\n",
      "Generated text 2: multiple girls, 2girls, red eyes, looking at another, ponytail, black neckwear, purple eyes, black skirt, very long hair, eye contact, silver hair, hand on another's face, bare shoulders, hair ribbon, red ribbon, pleated skirt, black choker, white shirt, sleeveless shirt, eyebrows visible through hair, black bow, bangs, collared shirt, sidelocks, medium breasts, school uniform, closed mouth, hand on another's cheek, miniskirt, red bow, black necktie, honkai impact 3, kiana/mei\n",
      "Generated text 3: 1girl, animal ears, blue eyes, black hair, holding umbrella, oil-paper umbrella, hair ornament, hair flower, looking at viewer, outdoors, bangs, black gloves, red flower, white flower, long sleeves, closed mouth, black kimono, blurry foreground, night, wide sleeves, depth of field, japanese clothes, white coat, dutch angle, animal ear fluff, very long hair, standing, from side, white umbrella, creation\n"
     ]
    }
   ],
   "source": [
    "from run_comfy_api import run_workflow, DEFAULT_NEG, DEFAULT_MODEL\n",
    "\n",
    "\n",
    "HARDCODED_MODEL_NAME = os.path.basename(os.path.normpath(MODEL_DIRS[0]))\n",
    "\n",
    "\n",
    "def gen_text_and_gen_image(rating: str, date: str, quality: str, character: str, prompt: str, max_length: int, \n",
    "                           seed:int, tags_front:str, tags_back:str):\n",
    "    \n",
    "    selected_models = [HARDCODED_MODEL_NAME]\n",
    "    \n",
    "    # one output, with single model\n",
    "    gend_text = generate_text(rating, date, quality, character, prompt, max_length, 4, selected_models)\n",
    "\n",
    "    gend_text = gend_text.split(\"\\n\")\n",
    "    gend_images = []\n",
    "\n",
    "    for i, txt in enumerate(gend_text):\n",
    "        print(f\"Generated text {i}: {txt}\")\n",
    "\n",
    "        # add quality tags\n",
    "\n",
    "        txt = \", \".join([tags_front, txt, tags_back])\n",
    "\n",
    "        _gend_images = run_workflow(pos=txt, \n",
    "                                neg=\"lowres, worst quality, displeasing, bad quality, bad anatomy, text, error, extra digit, cropped, average quality, 2000s\",\n",
    "                                seed = seed,\n",
    "                                batch_size=1,\n",
    "                                )\n",
    "        gend_images.extend(_gend_images)\n",
    "    \n",
    "    # Save images and metadata in the background\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        executor.submit(save_images_and_metadata, gend_images, gend_text, prompt)\n",
    "    \n",
    "    return \"\\n\\n\".join(gend_text), gend_images\n",
    "    \n",
    "\n",
    "\n",
    "# Define Gradio interface components\n",
    "checkbox_choices = [os.path.basename(os.path.normpath(model_dir)) for model_dir in MODEL_DIRS]\n",
    "iface = gr.Interface(\n",
    "    fn=gen_text_and_gen_image,\n",
    "    inputs=[\n",
    "        gr.Radio(choices=[\"general\", \"nsfw\"], label=\"Rating\", value=\"general\"),\n",
    "        gr.Radio(choices=[\"2000s\", \"2010s\", \"2020s\"], label=\"Date\", value=\"2020s\"),\n",
    "        gr.Radio(choices=[\"normal\", \"good\", \"excellent\"], label=\"Quality\", value=\"excellent\"),\n",
    "        gr.Textbox(lines=1, placeholder=\"hatsune miku\", label=\"Character tags\"),\n",
    "        gr.Textbox(lines=2, placeholder=\"1girl, long hair, looking at viewer\", label=\"General Tags\"),\n",
    "        gr.Slider(minimum=40, maximum=300, value=180, step=10, label=\"Max Prompt Length\"),\n",
    "        gr.Number(value=-1, label=\"Image Generation Seed\"),\n",
    "        gr.Textbox(lines=1, value=\"best quality, 2020s\", label=\"tags_front\"),\n",
    "        gr.Textbox(lines=1, value=\"[[absurdres]]\", label=\"tags_back\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Generated Texts\"),\n",
    "        gr.Gallery(label=\"Generated Images\", height=768),\n",
    "    ],\n",
    "    title=\"Prompt Augment and SDXL New Model Demo\",\n",
    "    description=\"\"\"\n",
    "Pipeline:\n",
    "  - input some danbooru tags (or danbooru-like tags)\n",
    "  - augment the input to match the model training distribution\n",
    "  - generate images from the augmented input (by calling ComfyUI API)\n",
    "  - receives output\n",
    "\n",
    "Input tags is in danbooru format (or similar): \n",
    "  - see: https://danbooru.donmai.us/posts/7793852\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
