{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "# https://www.it-jim.com/blog/training-and-fine-tuning-gpt-2-and-gpt-3-models-using-hugging-face-transformers-and-openai-api/\n",
    "\n",
    "\n",
    "MODEL_NAME = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The elf queen, the two red, gold-haired creatures that would go to the top of the arena, looked on with confusion. \"But our king, our lord has just arrived!\"\\n\\nThis time, they couldn\\'t predict the outcome or'}]\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(task='text-generation', model=MODEL_NAME, device='cpu')\n",
    "print(pipe('The elf queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc = {'input_ids': tensor([[  464, 23878, 16599]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "['The elf queen']\n"
     ]
    }
   ],
   "source": [
    "model = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "enc = tokenizer(['The elf queen'], return_tensors='pt')\n",
    "print('enc =', enc)\n",
    "print(tokenizer.batch_decode(enc['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.GPT2Config.from_pretrained(MODEL_NAME)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.do_sample = config.task_specific_params['text-generation']['do_sample']\n",
    "config.max_length = config.task_specific_params['text-generation']['max_length']\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The elf queen, an orphan boy with huge head and big hands, was about to become a godmother! When she saw my little fellow Elfling I knew something was going on as we watched her take one of our small children to a private high'}]\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(task='text-generation', model=MODEL_NAME, device='cpu')\n",
    "print(pipe('The elf queen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-16 03:25:37.739165: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-16 03:25:37.787484: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-16 03:25:38.495845: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "model = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 665/2500 [02:17<07:26,  4.11it/s]"
     ]
    }
   ],
   "source": [
    "# By IT-JIM, 2023\n",
    "# Train GPT-2 with PyTorch (no Trainer)\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import transformers\n",
    "import tqdm\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "MODEL_NAME = 'gpt2'\n",
    "TEXT_CORPUS = '/data/full_pixiv_harvest_prompt/prompts_1832.txt'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "TOKEN_ENDOFTEXT = 50256  # '<|endoftext|>\n",
    "BLOCK_LEN = 300\n",
    "BATCH_SIZE = 16  # Default batch size\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "def print_it(a, name: str = ''):\n",
    "    m = a.float().mean() if isinstance(a, torch.Tensor) else a.mean()\n",
    "    # m = a.mean()\n",
    "    print(name, a.shape, a.dtype, a.min(), m, a.max())\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "class MyDset(torch.utils.data.Dataset):\n",
    "    \"\"\"A custom dataset that serves 1024-token blocks as input_ids == labels\"\"\"\n",
    "    def __init__(self, data: list[list[int]]):\n",
    "        self.data = []\n",
    "        for d in data:\n",
    "            input_ids = torch.tensor(d, dtype=torch.int64)\n",
    "            attention_mask = torch.ones(len(d), dtype=torch.int64)\n",
    "            self.data.append({'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': input_ids})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "def break_text_to_lines(text_path: str, tokenizer: transformers.PreTrainedTokenizer, max_length: int) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Read a file line by line and convert each line to a tokenized list, \n",
    "    appending a TOKEN_ENDOFTEXT token to each. Truncate lines longer than max_length.\n",
    "\n",
    "    Args:\n",
    "    - text_path: Path to the text file.\n",
    "    - tokenizer: An instance of transformers.PreTrainedTokenizer.\n",
    "    - max_length: The maximum length for each tokenized line, including TOKEN_ENDOFTEXT.\n",
    "\n",
    "    Returns:\n",
    "    A list of tokenized lines, each as a list of integers.\n",
    "    \"\"\"\n",
    "    tokenized_lines = []\n",
    "    with open(text_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            tokens = tokenizer.encode(line.strip())[:max_length - 1]  # Leave space for TOKEN_ENDOFTEXT\n",
    "            tokens.append(TOKEN_ENDOFTEXT)  # Ensure the end token is added\n",
    "            tokenized_lines.append(tokens)\n",
    "    return tokenized_lines\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "def train_val_split(data: list[str], ratio: float):\n",
    "    n = len(data)\n",
    "    assert n >= 2\n",
    "    n_val = max(1, int(n * ratio))\n",
    "    return data[n_val:], data[:n_val]\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "def prepare_dsets(text_path: str, tokenizer: transformers.PreTrainedTokenizer, block_len: int):\n",
    "    \"\"\"Read the text, prepare the datasets \"\"\"\n",
    "    data = break_text_to_lines(text_path, tokenizer, block_len)\n",
    "    data_train, data_val = train_val_split(data, 0.2)\n",
    "    return MyDset(data_train), MyDset(data_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    A custom collate function for padding batches dynamically based on the max length\n",
    "    of the batch samples to allow for varying lengths of sequences within the same batch.\n",
    "    \"\"\"\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad the sequences to the maximum length in the batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)  # -100 is commonly used as ignore_index in PyTorch\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_masks_padded,\n",
    "        'labels': labels_padded\n",
    "    }\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "def train_one(model: torch.nn.Module, loader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer):\n",
    "    \"\"\"Standard PyTorch training, one epoch\"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    pbar = tqdm.tqdm(loader)\n",
    "    for batch in pbar:\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "        # loss, logits, past_key_values\n",
    "        loss = out['loss']\n",
    "        pbar.set_description(f'loss={loss.item()}')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "def val_one(model: torch.nn.Module, loader: torch.utils.data.DataLoader):\n",
    "    \"\"\"Standard PyTorch eval, one epoch\"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in tqdm.tqdm(loader):\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "        # loss, logits, past_key_values\n",
    "        loss = out['loss']\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "def generate_text(model, tokenizer, text, max_length=20, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generates text using the trained model and a starting text.\n",
    "    \n",
    "    Args:\n",
    "    - model: The trained model.\n",
    "    - tokenizer: The tokenizer for the model.\n",
    "    - text: Starting text for generation.\n",
    "    - max_length: The maximum length of the generated text.\n",
    "    - device: The device to run the generation on ('cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "    A string containing the generated text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch = tokenizer([text], return_tensors='pt')\n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "    generated_output = model.generate(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], max_length=max_length)\n",
    "    generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "\n",
    "# Initialize TensorBoard SummaryWriter\n",
    "writer = SummaryWriter('./runs/gpt2_training')\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create datasets and loader\n",
    "dset_train, dset_val = prepare_dsets(TEXT_CORPUS, tokenizer, BLOCK_LEN)\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(dset_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate)\n",
    "loader_val = torch.utils.data.DataLoader(dset_val, batch_size=BATCH_SIZE, collate_fn=pad_collate)\n",
    "\n",
    "\n",
    "# Optimizer, device\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "test_prompts = [\n",
    "    \"1girl, long hair, white hair\",\n",
    "    \"1girl, purple hair\",\n",
    "    \"gothic lolita\",\n",
    "    \"hatsune miku\",\n",
    "]\n",
    "\n",
    "# Training loop\n",
    "for i_epoch in range(20):\n",
    "    loss_train = train_one(model, loader_train, optimizer)\n",
    "    writer.add_scalar('Loss/train', loss_train, i_epoch)\n",
    "    \n",
    "    loss_val = val_one(model, loader_val)\n",
    "    writer.add_scalar('Loss/val', loss_val, i_epoch)\n",
    "    \n",
    "    print(f'{i_epoch} : loss_train={loss_train}, loss_val={loss_val}')\n",
    "\n",
    "    # genrate a sample\n",
    "    texts = []\n",
    "    for curr_prompt in test_prompts:\n",
    "        generated_text = generate_text(model, tokenizer, curr_prompt, 20, DEVICE)\n",
    "        texts.append(generated_text)\n",
    "\n",
    "    writer.add_text('Generated', '\\n'.join(texts), i_epoch)\n",
    "    print(f\"epoch {i_epoch} : {texts}\")\n",
    "    \n",
    "    # Save the model if needed\n",
    "    model.save_pretrained(f'./trained_model/epoch_{i_epoch}/')\n",
    "    tokenizer.save_pretrained(f'./trained_model/epoch_{i_epoch}/')    \n",
    "\n",
    "\n",
    "writer.close()\n",
    "\n",
    "\n",
    "\n",
    "# Now our model is trained, try the generation\n",
    "sample_text =  \"1girl, long hair, white hair\" \n",
    "generated_text = generate_text(model, tokenizer, sample_text, 20, DEVICE)\n",
    "print('GENERATION=', generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
